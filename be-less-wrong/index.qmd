---
title: "Be Less Wrong"
subtitle: "Why Use Advanced Quantitative Methods?"
author: "Andy Grogan-Kaylor"
date: "2024-12-17"
categories: [science, social justice, stats]
image: "math1.png"
draft: true
fig-height: 3
fig-dpi: 600
---

Across the globe, there are definite concrete *realities* of abuse, neglect, suffering, exploitation, violence, discrimination, and other associated problems that we are trying to understand, and to reduce. 

```{r}
#| label: fig-globe

library(plotly)

mymap <- plot_geo() %>%
  layout(geo = list(showland = TRUE, # show land
                    landcolor = toRGB("darkgrey"), # land color
                    showcountries = TRUE, # show countries
                    showocean = FALSE, # show ocean
                    oceancolor = "lightblue", # ocean color
                    lataxis = list(showgrid = TRUE, # latitude options
                                   gridcolor = toRGB("grey")),
                    lonaxis = list(showgrid = TRUE, # longitude options
                                   gridcolor = toRGB("grey")))) 

# mymap # replay

myglobe <- mymap %>% 
  layout(geo = list(projection = list(type = 'orthographic',
                                      rotation = list(lon = -45,
                                                      lat = 0,
                                                      roll = 0))))

myglobe # replay

```


> "What we see and how we see is of course determined by our perspective, by the place from which we begin our examination of history; but it is determined also by reality itself." [@Martin-Baro1994B]

We hope that our research will inform efforts to change these *realities*. Yet, at the same time we must recognize that our understandings are at best *iterative* and *contingent.* While we will never have a perfect understanding of social reality, we can always improve our understandings, and move in the direction of being *less wrong*.

As @Silverman1998 wrote: 

> "... there is no way to know when our observations about complex events in nature are complete. Our knowledge is finite, Karl Popper emphasised, but our ignorance is infinite. ... [W]e can never be certain about the consequences of our interventions, we can only narrow the area of uncertainty. This admission is not as pessimistic as it sounds: claims that resist repeated energetic challenges often turn out to be quite reliable. Such 'working truths' are the building blocks for the reasonably solid structures that support our everyday actions..." [@Silverman1998]

This recalls the famous saying by the statistician George Box about statistical models, reported in many places, and well captured in the passage below by @Hand2014:

> "In general, when building statistical models, we must not forget that the aim is to understand something about the real world. Or predict, choose an action, make a decision, summarize evidence, and so on, but always about the real world, not an abstract mathematical world: our models are not the realityâ€”a point well made by George Box in his oft-cited remark that 'all models are wrong, but some are useful' (Box, 1979 in @MR0554183)." [@Hand2014]

A key task then, of using quantitative methods is to use them to try to be progressively *less wrong* about the answers we are finding to important questions about improving human wellbeing. 

Let's consider some simple visual models based upon some simulated data. Two key variables in this model are *intervention* (a treatment or program that we hope does some good), and the *outcome* (an improved or beneficial mental health or psychological outcome).

```{r}

set.seed(1080)

library(ggplot2)

library(ggthemes)

library(gganimate)

library(patchwork)

```

```{r}
#| label: mydata

intervention <- seq(1,15) # intervention

group <- rep(seq(1,5), each=3) # group

index <- rep(seq(1,3), 5) # index within group

e <- rnorm(15, 0, .25) # randomly distributed error

outcome <- index + -.1 * intervention # + e

group <- factor(group)

mydata <- data.frame(intervention, group, outcome)

```


```{r}

p0 <- ggplot(mydata,
       aes(x = intervention,
           y = outcome,
           group = 1)) + 
  geom_point(size = 5)+
  geom_smooth(method="lm",
              linewidth = 2,
              se = FALSE) +
  scale_color_viridis_d(name = "group",
                        option = "turbo") +
  theme_minimal()

```


Here is a first model. What do these results say about the relationship of the *intervention* and the *outcome*?

```{r}
#| label: fig-p0

p0 + # replay
  labs(title = "A Simple Model of The Data",
       subtitle = "What is the observed relationship between intervention and outcome?",
       y = "beneficial mental health or \npsychological outcome") 

```

These simple straightforward results suggest that the *intervention* is associated with a worsening of the *outcome*.

::: {.callout-important appearance="simple"}
## The Intervention is *Not* Recommended

Based upon these results we would *not* recommend using this intervention.  
:::

Let's now consider a slightly more complex model. In addition to examining the *intervention* and the *outcome*, we account for the fact that individuals come from different *groups*. This could be any kind of group, e.g. a racial, ethnic, religious, cultural, or economic group. 

```{r}
#| label: fig-p0-group

p0 + 
  aes(color = group) +
  labs(title = "A Simple Model of The Data Accounting for Group Membership",
       subtitle = "What is the observed relationship between intervention and outcome?",
       y = "beneficial mental health or \npsychological outcome")


```

::: {.callout-important appearance="simple"}
## The Intervention is Recommended

Our conclusion seems to have flipped! Based upon these results we *would* recommend using this intervention.  
:::

The fact that statistical results--and analogously visual results--can flip when more variables are accounted for is known as Simpson's Paradox [@Simpson1951]. Put briefly, and intuitively, our evidence based "story" can change--sometimes quite dramatically--as we add more and more factors to our model.[^multilevelstructure]

[^multilevelstructure]: An analogous process can occur with *multilevel* data, in which there are often many groups, such as many schools, many neighborhoods, or many countries. Failure to account for the grouping of the data--in schools, neighborhoods or countries--can sometimes lead to dramatically incorrect results [@Gelman2007; @Nieuwenhuis2015]. 

::: {.callout-tip appearance="simple"}
## Include As Many Relevant Variables As You Can

Failure to include all of the relevant variables in our model--whether that model is visual or statistical--may lead to very wrong conclusions.

If those variables are observed, and included in our data set, it may be straightforward to build them into our model. If those variables are not observed, and not present in our data set, more complicated modeling strategies may be necessary.
:::

At first the scenario I've just presented seems almost like a trick, or a puzzle, designed to confound us, or to illustrate a convoluted statistical scenario. Yet, upon reflection, the scenario I've just presented is surprisingly plausible. 

::: {.callout-tip appearance="simple"}
## A Thought Experiment

Imagine a situation in which an intervention is administered based upon the situation in a local community. Quite possibly, the intervention might be given in communities where outcomes are less good. At the same time the intervention might be beneficial to individuals. Such a scenario would present us with exactly the data that we see reflected in @fig-p0 and @fig-p0-group.
:::

Let's think about one more of these scenarios. For the sake of parsimony, this time I'm going to present the two graphs together. 

```{r}
#| label: mydata2

intervention <- seq(1,15) # intervention

group <- rep(seq(1,5), each=3) # group

index <- rep(seq(1,3), 5) # index within group

e <- rnorm(15, 0, .25) # randomly distributed error

outcome <- -1 * index + .05 * intervention # + e

group <- factor(group)

mydata2 <- data.frame(intervention, group, outcome)

```

```{r}

p1 <- ggplot(mydata2,
       aes(x = intervention,
           y = outcome)) + 
  geom_point(size = 5)+
  geom_smooth(method="lm",
              linewidth = 2,
              se = FALSE) +
  scale_color_viridis_d(name = "group",
                        option = "turbo") +
  theme_minimal()

```

```{r}

p1A <- p1 + # replay
  labs(title = "A Simple Model of The Data",
       subtitle = "What is the observed relationship between intervention and outcome?",
       y = "beneficial mental health or \npsychological outcome")

```

```{r}

p1B <- p1 + 
  aes(color = group) +
  labs(title = "A Simple Model of The Data Accounting for Group Membership",
       subtitle = "What is the observed relationship between intervention and outcome?",
       y = "beneficial mental health or \npsychological outcome")


```

```{r}
#| label: fig-p1A

p1A

```

```{r}
#| label: fig-p1B

p1B

```

::: {.callout-tip appearance="simple"}
## Be Less Wrong

My point? Simple models feel intuitive and have a commonsense appeal. Yet, with even slightly complicated social issues, simple models may be wrong.
:::

What I have illustrated here is only one set of ideas about how we need to complicate our quantitative thinking to try to be a little less wrong in thinking about social problems. Other more advanced statistical techniques may be seen as attempts to deal with other complications of the data, in an effort to be *less wrong*. 

We need to keep the model as simple as possible so that it remains a useful abstraction, but to make the model complicated enough to reflect the complications of reality.










